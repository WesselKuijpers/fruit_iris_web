{
    "en": {
        "translations": {
            "page.index.upload.caption": "Tap to upload photo",
            "page.index.result.try-again.button": "Try Again!",
            "page.index.result.survey.title": "Help us out?",
            "page.index.result.survey.dropdown.default": "Did it work?",
            "page.index.result.survey.dropdown.positive": "yes",
            "page.index.result.survey.dropdown.negative": "no",
            "page.index.result.survey.answered.thank-you": "Thank you!",
            "page.index.result.survey.answered.dropdown.default": "Then what is it?",
            "base.navbar.link.home": "home",
            "base.navbar.link.insight": "insights",
            "base.navbar.link.rate": "rate",
            "insight.jumbotron.title": "Argos IRIS Insights",
            "insight.jumbotron.lead": "This page contains all the information about the inner workings of Argos IRIS, as well as metrics calculated on the model. <br> The data on this page is constantly updated with the latest metrics, because the model is constantly adapting to new data.",
            "insight.accuracy-and-loss.intro.title": "Accuracy and Loss",
            "insight.accuracy-and-loss.intro.lead": "Below you will find the metrics for the accuracy and loss of the current model during the training process. <br> The lines labled 'training accuracy' and 'training loss' contain the metrict during the actual process with data that the model has 'seen' before. <br> The lines labled 'validation accuracy' and 'validation loss' show how the model preformed in predicting data that was never seen before. <br> Ideally, the validation line should closely follow the training line.",
            "insight.accuracy-and-loss.accuracy.title": "Accuracy",
            "insight.accuracy-and-loss.accuracy.subtitle": "Higher is better, in percentage",
            "insight.accuracy-and-loss.accuracy.explanation": "The accuracy of a model is usually determined after the model parameters are learned and fixed and no learning is taking place. Then the test samples are fed to the model and the number of mistakes (zero-one loss) the model makes are recorded, after comparison to the true targets. Then the percentage of misclassification is calculated. <br> For example, if the number of test samples is 1000 and model classifies 952 of those correctly, then the model's accuracy is 95.2%.",
            "insight.accuracy-and-loss.loss.title": "Loss",
            "insight.accuracy-and-loss.loss.subtitle": "Lower is better",
            "insight.accuracy-and-loss.loss.explanation": "Loss value implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of loss after each, or several, iteration(s). <br> Loss is often used in the training process to find the 'best' parameter values for your model (e.g. weights in neural network). It is what you try to optimize in the training by updating weights.",
            "insight.classification-report.intro.title": "Classification Report",
            "insight.classification-report.intro.lead": "When a training process is finished a report is generated to validate the precision of the model. <br> This report is generated by running 10402 images the model has never seen before by it. The model then gives it's prediction, the outcome of this, matched with the actual label of the respective images, is fed into several mathematical functions returning the metrics below.",
            "insight.classification-report.support.title": "Support",
            "insight.classification-report.support.subtitle": "How many images exist inside each category of the test set",
            "insight.classification-report.precision.title": "Per class precision",
            "insight.classification-report.precision.subtitle": "in percentages",
            "insight.classification-report.precision.explanation": "Precision – Accuracy of positive predictions. <br> The precision is being calculated with the following formula:",
            "insight.classification-report.precision.formula":  "Precision = True Positives / (True Positives + False Positives)",
            "insight.classification-report.recall.title": "Per class recall score",
            "insight.classification-report.recall.subtitle": "in percentages",
            "insight.classification-report.recall.explanation": "Recall (sensitivity or true positive rate) – Fraction of positives That were correctly identified. <br> The Recall Score is being calculated with the following formula:",
            "insight.classification-report.recall.formula": "Precision = True Positives / (True Positives + False Negatives)",
            "insight.classification-report.f1.title": "Per class F1 score",
            "insight.classification-report.f1.subtitle": "in percentages",
            "insight.classification-report.f1.explanation": "F1 Score (F-Score or F-Measure) – A metric for comparing two classifiers. F1 Score takes into account precision and the recall. It is created by finding the the harmonic mean of precision and recall. <br> The F1 Score is being calculated with the following formula:",
            "insight.classification-report.f1.formula": "F1 = 2 x (precision x recall)/(precision + recall)",
            "insight.confusion-matrix.intro.title": "Confusion Matrix",
            "insight.confusion-matrix.intro.lead": "When a training process is finished a confusion matrix is generated to give a better insight in the precision of the model. <br> The confusion matrix shows how often certain classes are predicted and how images are misclassified. Ideally a digagonal 'line' should be discernable in the confusion matrix.",
            "insight.confusion-matrix.table.title": "Confusion Matrix",
            "insight.confusion-matrix.table.subtitle": "A matrix, labeled by class"
        },
        "fullname": "English"
    },
    "nl": {
        "translations": {
            "page.index.upload.caption": "Tik om foto te uploaden",
            "page.index.result.try-again.button": "Opnieuw!",
            "page.index.result.survey.title": "Handje helpen?",
            "page.index.result.survey.dropdown.default": "Klopt de voorspelling?",
            "page.index.result.survey.dropdown.positive": "ja",
            "page.index.result.survey.dropdown.negative": "nee",
            "page.index.result.survey.answered.thank-you": "Bedankt!",
            "page.index.result.survey.answered.dropdown.default": "Wat is het dan?",
            "base.navbar.link.home": "home",
            "base.navbar.link.insight": "statistieken",
            "base.navbar.link.rate": "beoordelen",
            "insight.jumbotron.title": "Argos IRIS statistieken",
            "insight.jumbotron.lead": "Deze pagina bevat informatie over de werking van Argos IRIS, evenals statistieken berekend op basis van het gebruikte model. <br> De data op deze pagina wordt constant geüpdate omdat het model zich constant aanpast aan nieuwe data.",
            "insight.accuracy-and-loss.intro.title": "'Accuracy' en 'Loss'",
            "insight.accuracy-and-loss.intro.lead": "Hieronder staan statisitieken over de 'accuracy' en 'loss' van het huidige model tijdens de laatste trainingsperiode. <br> De lijnen met label 'training accuracy' en 'training loss' bevatten de gevens tijdens het daadwerkelijke proces met data die het model al eerder heeft 'gezien'. <br> De lijnen met labels 'validation accuracy' en 'validation loss' laten zien hoe goed het model presteert tijdens het voorspellen van data die het nog niet eerder heeft 'gezien'. <br> In de ideale situatie volgt de validatielijn de trainingslijn op de voet.",
            "insight.accuracy-and-loss.accuracy.title": "Accuracy",
            "insight.accuracy-and-loss.accuracy.subtitle": "Hoe hoger, hoe beter. Een percentage",
            "insight.accuracy-and-loss.accuracy.explanation": "De accuracy van een model wordt over het algemeen vastgesteld nadat de parameters van het model vastgelegd en geleerd zij en er niet meer geleerd wordt. De test data wordt aan het model gegeven en het aantal fouten (zero-one loss) die het model maakt worden opgeslagen, nadat de vergelijking met de 'echte' labels is gemaakt. Daarna wordt het percentage van misvattingen berekend. <br> Een voorbeeld: als het aantal testafbeeldingen 1000 is en het model classificeerd 952 van deze correct, dan is de 'accuracy' van het model 95.2%.",
            "insight.accuracy-and-loss.loss.title": "Loss",
            "insight.accuracy-and-loss.loss.subtitle": "Hoe lager, hoe beter",
            "insight.accuracy-and-loss.loss.explanation": "'Loss' waarder impliceert hoe 'goed' of 'slecht' het model presteert na elke iteratie van optimalisatie. Idealiter is te verwachten dat deze waarde na elke iteratie afneemt. <br> Loss wordt gebruikt om de 'beste' parameters voor modeltraining the vinden (bijvoorbeeld 'weights' in neuraal netwerk). Dit optimalizeren gebeurt door de 'weights' te updaten.",
            "insight.classification-report.intro.title": "Classificatie rapport",
            "insight.classification-report.intro.lead": "Wanneer een trainingsprocess wort afgesloten wordt er automatisch een classificatieraport gegenereerd om de prestatie van het model te kunnen meten. <br> Dit rapport wordt gegenereerd door 10402 afbeeldingen, die het model nog nooit gezien heeft, aan het model te laten zien. Het model geeft zijn voorspelling, de uitkomst hiervan, gepaard met het 'echte' bijbehpordende label, wordt gebruikt in verschillende wiskundige functies. Dit levert de onderstaande statistieken op.",
            "insight.classification-report.support.title": "Support",
            "insight.classification-report.support.subtitle": "Hoeveel afbeeldingen bevat elke categorie van de testdataset.",
            "insight.classification-report.precision.title": "Precisie per klasse",
            "insight.classification-report.precision.subtitle": "Een percentage",
            "insight.classification-report.precision.explanation": "Precisie – nauwkeurigheid van positieve voorspellingen. <br> De precisie wordt berekend aan de hand van de volgende formule:",
            "insight.classification-report.precision.formula":  "Precisie = terechte positieven / (terechte positieven + onterechte positieven)",
            "insight.classification-report.recall.title": "'Recall' -score per klasse",
            "insight.classification-report.recall.subtitle": "Een percentage",
            "insight.classification-report.recall.explanation": "Recall (gevoeligheid of mate van terechte positieven) – Fractie van de positieven de juist geïdentificeerd zijn. <br> De 'recall' score wordt met de volgende formule berekend:",
            "insight.classification-report.recall.formula": "Recall = terechte positieven / (terechte positieven + onterechte negatieven)",
            "insight.classification-report.f1.title": "F1-score per klasse",
            "insight.classification-report.f1.subtitle": "Een percentage",
            "insight.classification-report.f1.explanation": "F1 Score – Een methode om twee waarden te vergelijken. F1-Score neemt zowel de precision als de 'recall' in acht. Ze wordt gevormd door de harmonie in de precisie en 'recall' te vinden. <br> De F1-score wordt volgens de volgende formule berekend:",
            "insight.classification-report.f1.formula": "F1 = 2 x (precisie x recall)/(precisie + recall)",
            "insight.confusion-matrix.intro.title": "Confusion Matrix",
            "insight.confusion-matrix.intro.lead": "Wanneer een trainprocess voltooid is wordt er een confusion matrix gegenereed, deze geeft inzicht in de voorspellingen die door het model gemaakt worden. Idealiter zou er een diagonale 'lijn' in de matrix zichtbaar moeten zijn.",
            "insight.confusion-matrix.table.title": "Confusion Matrix",
            "insight.confusion-matrix.table.subtitle": "Een matrix, per klasse"
        },
        "fullname": "Nederlands"
    }
}